{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtQUtwVqNt3Ezb1Cw06Iws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prashantlondhe123/Cloud-Data-Engineering-/blob/main/Data_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Engineering:\n",
        "- Data engineering is the practice of\n",
        " designing and building systems for collecting, storing, and\n",
        "analyzing data at scale.\n",
        "\n",
        "-  Data engineers work in a variety of\n",
        " settings to build systems that collect, manage, and convert raw \n",
        "data into usable information for data scientists and business analysts to interpret"
      ],
      "metadata": {
        "id": "4dFGWnMjCN1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cloud Data Engineering: \n",
        "A cloud data engineer, also known as a cloud engineer or cloud developer, is someone responsible \n",
        "for the management of corporate apps and data in the cloud and all the technical tasks involved in\n",
        "planning, architecting, migrating, monitoring, and managing a company's cloud systems\n",
        "\n",
        "#Cloud tools for data engineering:\n",
        "-Adf\n",
        "\n",
        "-Azure databicks\n",
        "\n",
        "-Adls\n",
        "\n",
        "-Storage account\n",
        "\n",
        "-Azure Synapse \n",
        "\n",
        "-Azure logic app\n",
        "\n",
        "-Power Bi\n",
        "\n",
        "-Model building"
      ],
      "metadata": {
        "id": "9nmEUYMYCgMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Structured, Semi structure, Unstructure data:\n",
        "**Structured:** csv ,excel , database, Parquet/orc/avro can we structured and semi-structured\n",
        "\n",
        "**Unstructured:** text, video ,audio, images, number, messages, social media post, survey forms,\n",
        "\n",
        "**Semi structured data:** email, www,xml,pdf"
      ],
      "metadata": {
        "id": "RbmQYRZmDDJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ETL:\n",
        "   ETL is an abbreviation of Extract, Transform and Load. In this process, an ETL tool extracts the data from different RDBMS source systems then transforms the data like applying calculations, concatenations, etc. and then load the data into the Data Warehouse system"
      ],
      "metadata": {
        "id": "10pzqrWsDb5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELT:\n",
        "- ELT is a different method of looking at the tool approach to data movement. Instead of transforming the data before it’s written, ELT lets the target system to do the transformation. The data first copied to the target and then transformed in place.\n",
        "-ELT usually used with no-Sql databases like Hadoop cluster, data appliance or cloud installation."
      ],
      "metadata": {
        "id": "WpODEzM4DoG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OLTP:\n",
        "- OLTP or Online Transaction Processing is a type of data processing that consists of executing a number of transactions occurring concurrently—online banking, shopping, order entry, or sending text messages,\n",
        "for example. These transactions traditionally are referred to as economic or financialtransactions,.recorded and secured so that an enterprise can access the information anytime for accounting or reporting purposes"
      ],
      "metadata": {
        "id": "BiCaziexD4ZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#OLAP:\n",
        "- OLAP on big data is a powerful concept that involves the pre-aggregation of massive amounts of data \n",
        "and builds multidimensional cubes to get super-fast query results"
      ],
      "metadata": {
        "id": "JOgIcEGQENs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "JZJ-8M5QEcYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Cosmosdb:\n",
        " Azure Cosmos DB is Microsoft’s globally distributed, multi-model database. Azure Cosmos DB enables you to elastically and\n",
        "independently scale throughput and storage across any number of Azure’s geographic regions. It offers throughput, latency, \n",
        "availability, and consistency guarantees with comprehensive service level agreements (SLAs).\n",
        "\n",
        "**Azure Cosmos DB provides APIs for\n",
        "the following data models, with SDKs available in multiple languages**\n",
        "- SQL API\n",
        "- MongoDB API\n",
        "- Cassandra API\n",
        "- Graph (Gremlin) API\n",
        "- Table API"
      ],
      "metadata": {
        "id": "fYhcZYkbFKsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Cassandra:\n",
        " Apache Cassandra is an open-source no SQL database that is used for handling big data. Apache Cassandra has the\n",
        "capability to handle structure, semi-structured, and unstructured data. Apache Cassandra was originally developed at Facebook after\n",
        "that it was open-sourced in 2008 and after that, it become one of the top-level Apache projects in 2010.\n",
        "\n",
        "- It is scalable, fault-tolerant, and consistent.\n",
        "- It is column-oriented database.\n",
        "- Its distributed design is based on Amazon’s Dynamo and its data model on Google’s Big table.\n",
        "- It is Created at Facebook and it differs sharply from relational database management systems\n",
        "\n",
        "**Features of Cassandra:**\n",
        "- Easy data distribution\n",
        "- Flexible data storage\n",
        "- Elastic scalability\n",
        "- Fast writes\n",
        "- Always on Architecture\n",
        "- Fast linear-scale performance\n",
        "- Transaction support"
      ],
      "metadata": {
        "id": "ekNItBLFGo0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache hive:\n",
        "- Apache Hive is open-source data warehouse software designed to read, write, and manage large datasets extracted from the Apache Hadoop Distributed File System (HDFS) , one aspect of a larger\n",
        " Hadoop Ecosystem.\n",
        "- Apache Hive is an open source project that was conceived of by co-creators Joydeep Sen Sarma and\n",
        " Ashish Thusoo during their time at Facebook\n",
        "\n",
        "**Apache Hive architecture and key Apache Hive components:**\n",
        "\n",
        "**1)Hive Server 2:**\n",
        "The Hive Server 2 accepts incoming requests from users and applications and creates an execution plan \n",
        "and auto generates a YARN job to process SQL queries. The server also supports the Hive optimizer and \n",
        "Hive compiler to streamline data extraction and processing.\n",
        "\n",
        "**2)Hive Query Language:**\n",
        "By enabling the implementation of SQL-reminiscent code, the Apache Hive negates the need for long-winded \n",
        "JavaScript codes to sort through unstructured data and allows users to make queries using built-in HQL\n",
        " statements (HQL). These statements can be used to navigate large datasets, refine results, and share data\n",
        " in a cost-effective and time-efficient manner.\n",
        "\n",
        "**3)The Hive Metastore:**\n",
        "The central repository of the Apache Hive infrastructure, the metastore is where all of the Hive’s \n",
        "metadata is stored. In the metastore, metadata can also be formatted into Hive tables and partitions\n",
        "to compare data across relational databases. This includes table names, column names, data types,\n",
        "partition information, and data location on HDFS.\n",
        "\n",
        "**4)Hive Beeline Shell:**\n",
        "In line with other database management systems (DBMS), Hive has its own built-in command-line interface\n",
        "where users can run HQL statements. Also, the Hive shell also runs Hive JDBC and ODBC drivers and so\n",
        "can conduct queries from an Open Database Connectivity or Java Database Connectivity application.\n",
        "\n",
        "\n",
        "**What are the five different data types used by Apache Hive?:**\n",
        "**1)Numeric Data Types:**\n",
        "As the name suggests, these data types are integer-based data types. Examples of these data types are \n",
        "‘TINYINT,’ ‘SMALLINT,’ ‘INT,’ and ‘BIGINT’.\n",
        "\n",
        "**2)Date/Time Data Types:**\n",
        "These data types allow users to input a time and a date, with ‘TIMESTAMP,’ ‘DATE,’ and ‘INTERVAL,’ \n",
        "all being accepted inputs.\n",
        "\n",
        "**3)String Data Types:**\n",
        "Again this type of data is very straightforward and allows for written text, or ‘strings,’ to be \n",
        "implemented as data for processing. String data types include ‘STRING,’ ‘VARCHAR,’ and ‘CHAR.’\n",
        "\n",
        "**4)Complex Data Types:**\n",
        "One of the more advanced data types, complex types record more elaborate data and consist of types\n",
        " like ‘STRUCT’, ‘MAP,’ ‘ARRAY,’ and ‘UNION.’\n",
        "\n",
        "**5)Misc. Types:**\n",
        "Data types that don’t fit into any of the other four categories are known as miscellaneous data \n",
        "types and can take inputs such as ‘BOOLEAN’ or ‘BINARY.’"
      ],
      "metadata": {
        "id": "3ZhfVvyKHKXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hbase:\n",
        "- HBase is a column-oriented non-relational database management system that runs on top of Hadoop \n",
        "Distributed File System (HDFS). HBase provides a fault-tolerant way of storing sparse data sets,\n",
        " which are common in many big data use cases. It is well suited for real-time data processing or\n",
        " random read/write access to large volumes of data.\n",
        "- HBase is a data model that is similar to Google’s big table designed to provide quick random access\n",
        " to huge amounts of structured data. It leverages the fault tolerance provided by the Hadoop File\n",
        " System (HDFS).\n",
        "- It is a part of the Hadoop ecosystem that provides random real-time read/write access to data in \n",
        "the Hadoop File System"
      ],
      "metadata": {
        "id": "RO1f3gciId29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache kafka:\n",
        "Apache Kafka is an open-source distributed event streaming platform used by thousands of companies \n",
        "for high-performance data pipelines, streaming analytics, data integration, and mission-critical\n",
        " applications"
      ],
      "metadata": {
        "id": "o-WkmJQ6IpcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apache Snowflake:\n",
        "- Snowflake is a single platform comprised of storage, compute, and services layers that are logically\n",
        " integrated but scale infinitely and independent from one another.\n",
        "- Snowflake is a cloud data warehouse built on top of the public cloud (AWS/Azure / GCP) infrastructure\n",
        " and is a true SaaS offering. There is no hardware (virtual or physical) for you to select, install,\n",
        " configure, or manage. There is no software for you to install, configure, or manage. All ongoing\n",
        " maintenance, management, and tuning is handled by Snowflake."
      ],
      "metadata": {
        "id": "Rsb3TJJbIvP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Difference among data warehouse, data lake, Delta lake\n",
        "**1] DATA WAREHOUSE:**\n",
        "- Only Structured Data\n",
        "- Schema-on-Write\n",
        "- Supports ACID Transaction\n",
        "- Does not corrupt the system\n",
        "\n",
        "**2]Data Lake:**\n",
        "- Structured/Semi structured/Unstructured\n",
        "- Schema-on-Read\n",
        "- Minimal support to ACID Transactions\n",
        "- Leaves system in corrupted state\n",
        "\n",
        "**3]Delta lake:**\n",
        "- Structured/Semi structured/Unstructured/ Streaming\n",
        "- bSchema-on-Read\n",
        "- Supports ACID Transaction\n",
        "- Does not corrupt the system"
      ],
      "metadata": {
        "id": "jwYLhMJUI6tZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0h5zgXuuJX8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Formats:\n",
        "**1]CSV:** Good option for compatibility, spreadsheet processing and human readable data. The data must be \n",
        "flat. It is not efficient and cannot handle nested data. There may be issues with the separator which\n",
        "can lead to data quality issues. Use this format for exploratory analysis, POCs or small data sets.\n",
        "\n",
        "**2]JSON:** Heavily used in APIs. Nested format. It is widely adopted and human readable but it can be\n",
        "difficult to read if there are lots of nested fields. Great for small data sets, landing data or\n",
        "API integration. If possible convert to more efficient format before processing large amounts of data.\n",
        "\n",
        "**3]Avro:** Great for storing row data, very efficient. It has a schema and supports evolution. Great\n",
        "integration with Kafka. Supports file splitting. Use it for row level operations or in Kafka.\n",
        "Great to write data, slower to read.\n",
        "\n",
        "**4]Protocol Buffers:** Great for APIs, especially for gRPC. Supports Schema and it is very fast. Use \n",
        "for APIs or machine learning.\n",
        "\n",
        "**5]Parquet:** Columnar storage. It has schema support. It works very well with Hive and Spark as a\n",
        "way to store columnar data in deep storage that is queried using SQL. Because it stores data\n",
        "in columns, query engines will only read files that have the selected columns and not the entire \n",
        "data set as opposed to Avro. Use it as a reporting layer.\n",
        "\n",
        "**6]ORC:** Similar to Parquet, it offers better compression. It also provides better schema evolution\n",
        " support as well, but it is less popular.\n",
        "\n",
        "**1] Avro:**\n",
        "Schema Evolution -Best\n",
        "Compression - Good\n",
        "Splitability - Good\n",
        "Row or Column - Row\n",
        "Read or Write- Write\n",
        "\n",
        "**2] Parquet:**\n",
        "Schema Evolution -Good\n",
        "Compression - Better\n",
        "Splitability - Good\n",
        "Row or Column - Column\n",
        "Read or Write-  Read\n",
        "\n",
        "**3] ORC:**\n",
        "Schema Evolution -Better\n",
        "Compression - best\n",
        "Splitability - best\n",
        "Row or Column - column\n",
        "Read or Write- read"
      ],
      "metadata": {
        "id": "aYkW38BzKEF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ayw0pyOTJsJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hvN4XpsGCd-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G50cTfC6CL3s"
      }
    }
  ]
}