{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN37B/bhbxfPfMgsjwsOGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prashantlondhe123/Cloud-Data-Engineering-/blob/main/Data_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Engineering:\n",
        "- Data engineering is the practice of\n",
        " designing and building systems for collecting, storing, and\n",
        "analyzing data at scale.\n",
        "\n",
        "-  Data engineers work in a variety of\n",
        " settings to build systems that collect, manage, and convert raw \n",
        "data into usable information for data scientists and business analysts to interpret"
      ],
      "metadata": {
        "id": "4dFGWnMjCN1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the 5 V’s in Big Data?\n",
        "\n",
        "**1)Volume:**  A considerable amount of data stored in data warehouses reflects the volume. The data may reach random heights; these large volumes of data need to be examined and processed. Which may exist up to or more than terabytes and petabytes.\n",
        "\n",
        "**2)Velocity:** Velocity basically introduces the pace at which data is being produced in real-time. To give a simple example for recognition, imagine the rate at which Facebook, Instagram, or Twitter posts are generated per second, an hour or more.\n",
        "\n",
        "**3)Variety:**  Big Data comprises structured, unstructured, and semi-structured data collected from varied sources. This different variety of data requires very different and specific analyzing and processing techniques with unique and appropriate algorithms.\n",
        "\n",
        "**4)Veracity:** Data veracity basically relates to how reliable the data is, or in a fundamental way, we can define it as the quality of the data analyzed.\n",
        "\n",
        "**5)Value:** Raw data is of no use or meaning but once converted into something valuable. We can extract helpful information."
      ],
      "metadata": {
        "id": "jQLRLvNLNUR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why businesses are using Big Data for competitive advantage.\n",
        "\n",
        "Irrespective of the division and scope of the firm, data is now an essential tool for businesses to utilise. Companies are frequently using big data to gain a competing edge over business rivals.\n",
        "\n",
        "Checking the datasets a company collects is just one part of the big data process. Big data professionals also need to know what the company requires from the application and how they plan to use the data to their advantage"
      ],
      "metadata": {
        "id": "gI9r2vtOOGqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to deploy a Big Data Model? Mention the key steps involved.\n",
        "Deploying a model into a Big Data Platform involves mainly three key steps they are,\n",
        "\n",
        "- Data ingestion\n",
        "- Data Storage\n",
        "- Data Processing\n",
        "\n",
        "**Let’s have a look at what these are,**\n",
        "\n",
        "**Data Ingestion:** This process involves collecting data from different sources like social media platforms, business applications, log files, etc.\n",
        "\n",
        "**Data Storage:** When data extraction is completed, the challenge is to store this large volume of data in the database in which the Hadoop Distributed File system (HDFS) plays a vital role.\n",
        "\n",
        "**Data Processing:** After storing the data in HDFS or HBase, the next task is to analyze and visualize these large amounts of data using specific algorithms for better data processing. And yet again, this task is more straightforward if we use Hadoop, Apache Spark, Pig, etc.\n",
        "After performing these essential steps, one can deploy a big data model successfully"
      ],
      "metadata": {
        "id": "RyekbmZjOhTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the different big data processing techniques?\n",
        "Big Data processing methods analyze big data sets at a massive scale. Offline batch data processing is typically full power and full scale, tackling arbitrary BI scenarios. In contrast, real-time stream processing is conducted on the most recent slice of data for data profiling to pick outliers, impostor transaction exposures, safety monitoring, etc. However, the most challenging task is to do fast or real-time ad-hoc analytics on a big comprehensive data set. It substantially means you need to scan tons of data within seconds. This is only probable when data is processed with high parallelism.\n",
        "\n",
        "**Different techniques of Big Data Processing are:**\n",
        "\n",
        "- Batch Processing of Big Data\n",
        "- Big Data Stream Processing \n",
        "- Real-Time Big Data Processing\n",
        "- Map Reduce"
      ],
      "metadata": {
        "id": "lsLYdfiYQI1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain Features Selection.\n",
        " During processing, Big data may contain a large amount of data that is not required at a particular time, So we may be required to select only some specific features that we are interested in. The process of extracting only the needed features from the Big data is called Feature selection.\n",
        "\n",
        "**Feature selection Methods are -**\n",
        "\n",
        "**Filters Method:** In this method of variable ranking, we only consider the importance and usefulness of a feature.\n",
        "\n",
        "**Wrappers Method:** In this method, ‘induction algorithm’ is used, Which can be used to produce a classifier.\n",
        "**Embedded Method:** This method is a combination of efficiencies of both Filters and wrappers methods"
      ],
      "metadata": {
        "id": "yOekZo_HQgmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do you convert unstructured data to structured data?\n",
        "An open-ended question and there are many ways to achieve this.\n",
        "\n",
        "**Programming:**  Coding/ Programming is the most tried out method to transform unstructured data into a structured form. Programming is advantageous to accomplish because we get independence with it, which you can use to change the structure of the data in any form possible. Several programming languages, such as Python, Java, etc., can be used.\n",
        "**Data/Business Tools:** Many BI (Business Intelligence) tools support the drag and drop functionality for converting unstructured data into structured data. One cautious thing before using BI tools is that most of these tools are paid, and we have to be financially capable to support these tools. For people who lack both experience and skills needed for option 1, this is the way to go."
      ],
      "metadata": {
        "id": "zNdf-VpTSAcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is data preparation?\n",
        "Data preparation is the method of cleansing and modifying raw data before processing and analyzing it. It is a crucial step before processing and usually requires reformatting data, making improvements to data, and consolidating data sets to enrich data.\n",
        "\n",
        "Data preparation is an unending task for data specialists or business users. But, it is essential to convert data into context to get insights and then, can eliminate the biased results found due to poor data quality.\n",
        "\n",
        "For instance, the data construction process typically includes standardizing data formats, enhancing source data, and/or eliminating outliers\n",
        "\n",
        "- Gather data\n",
        "- Discover and assess data\n",
        "- Clean and verify data\n",
        "- Transform and enrich data\n",
        "- Store data"
      ],
      "metadata": {
        "id": "1u7KSQbFTkCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VT7BE-n0P0wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cloud Data Engineering: \n",
        "A cloud data engineer, also known as a cloud engineer or cloud developer, is someone responsible \n",
        "for the management of corporate apps and data in the cloud and all the technical tasks involved in\n",
        "planning, architecting, migrating, monitoring, and managing a company's cloud systems\n",
        "\n",
        "#Cloud tools for data engineering:\n",
        "-Adf\n",
        "\n",
        "-Azure databicks\n",
        "\n",
        "-Adls\n",
        "\n",
        "-Storage account\n",
        "\n",
        "-Azure Synapse \n",
        "\n",
        "-Azure logic app\n",
        "\n",
        "-Power Bi\n",
        "\n",
        "-Model building"
      ],
      "metadata": {
        "id": "9nmEUYMYCgMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Structured, Semi structure, Unstructure data:\n",
        "**Structured:** csv ,excel , database, Parquet/orc/avro can we structured and semi-structured\n",
        "\n",
        "**Unstructured:** text, video ,audio, images, number, messages, social media post, survey forms,\n",
        "\n",
        "**Semi structured data:** email, www,xml,pdf"
      ],
      "metadata": {
        "id": "RbmQYRZmDDJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ETL:\n",
        "   ETL is an abbreviation of Extract, Transform and Load. In this process, an ETL tool extracts the data from different RDBMS source systems then transforms the data like applying calculations, concatenations, etc. and then load the data into the Data Warehouse system"
      ],
      "metadata": {
        "id": "10pzqrWsDb5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELT:\n",
        "- ELT is a different method of looking at the tool approach to data movement. Instead of transforming the data before it’s written, ELT lets the target system to do the transformation. The data first copied to the target and then transformed in place.\n",
        "-ELT usually used with no-Sql databases like Hadoop cluster, data appliance or cloud installation."
      ],
      "metadata": {
        "id": "WpODEzM4DoG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OLTP:\n",
        "- OLTP or Online Transaction Processing is a type of data processing that consists of executing a number of transactions occurring concurrently—online banking, shopping, order entry, or sending text messages,\n",
        "for example. These transactions traditionally are referred to as economic or financialtransactions,.recorded and secured so that an enterprise can access the information anytime for accounting or reporting purposes"
      ],
      "metadata": {
        "id": "BiCaziexD4ZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#OLAP:\n",
        "- OLAP on big data is a powerful concept that involves the pre-aggregation of massive amounts of data \n",
        "and builds multidimensional cubes to get super-fast query results"
      ],
      "metadata": {
        "id": "JOgIcEGQENs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "JZJ-8M5QEcYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Cosmosdb:\n",
        " Azure Cosmos DB is Microsoft’s globally distributed, multi-model database. Azure Cosmos DB enables you to elastically and\n",
        "independently scale throughput and storage across any number of Azure’s geographic regions. It offers throughput, latency, \n",
        "availability, and consistency guarantees with comprehensive service level agreements (SLAs).\n",
        "\n",
        "**Azure Cosmos DB provides APIs for\n",
        "the following data models, with SDKs available in multiple languages**\n",
        "- SQL API\n",
        "- MongoDB API\n",
        "- Cassandra API\n",
        "- Graph (Gremlin) API\n",
        "- Table API"
      ],
      "metadata": {
        "id": "fYhcZYkbFKsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Cassandra:\n",
        " Apache Cassandra is an open-source no SQL database that is used for handling big data. Apache Cassandra has the\n",
        "capability to handle structure, semi-structured, and unstructured data. Apache Cassandra was originally developed at Facebook after\n",
        "that it was open-sourced in 2008 and after that, it become one of the top-level Apache projects in 2010.\n",
        "\n",
        "- It is scalable, fault-tolerant, and consistent.\n",
        "- It is column-oriented database.\n",
        "- Its distributed design is based on Amazon’s Dynamo and its data model on Google’s Big table.\n",
        "- It is Created at Facebook and it differs sharply from relational database management systems\n",
        "\n",
        "**Features of Cassandra:**\n",
        "- Easy data distribution\n",
        "- Flexible data storage\n",
        "- Elastic scalability\n",
        "- Fast writes\n",
        "- Always on Architecture\n",
        "- Fast linear-scale performance\n",
        "- Transaction support"
      ],
      "metadata": {
        "id": "ekNItBLFGo0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache hive:\n",
        "- Apache Hive is open-source data warehouse software designed to read, write, and manage large datasets extracted from the Apache Hadoop Distributed File System (HDFS) , one aspect of a larger\n",
        " Hadoop Ecosystem.\n",
        "- Apache Hive is an open source project that was conceived of by co-creators Joydeep Sen Sarma and\n",
        " Ashish Thusoo during their time at Facebook\n",
        "\n",
        "**Apache Hive architecture and key Apache Hive components:**\n",
        "\n",
        "**1)Hive Server 2:**\n",
        "The Hive Server 2 accepts incoming requests from users and applications and creates an execution plan \n",
        "and auto generates a YARN job to process SQL queries. The server also supports the Hive optimizer and \n",
        "Hive compiler to streamline data extraction and processing.\n",
        "\n",
        "**2)Hive Query Language:**\n",
        "By enabling the implementation of SQL-reminiscent code, the Apache Hive negates the need for long-winded \n",
        "JavaScript codes to sort through unstructured data and allows users to make queries using built-in HQL\n",
        " statements (HQL). These statements can be used to navigate large datasets, refine results, and share data\n",
        " in a cost-effective and time-efficient manner.\n",
        "\n",
        "**3)The Hive Metastore:**\n",
        "The central repository of the Apache Hive infrastructure, the metastore is where all of the Hive’s \n",
        "metadata is stored. In the metastore, metadata can also be formatted into Hive tables and partitions\n",
        "to compare data across relational databases. This includes table names, column names, data types,\n",
        "partition information, and data location on HDFS.\n",
        "\n",
        "**4)Hive Beeline Shell:**\n",
        "In line with other database management systems (DBMS), Hive has its own built-in command-line interface\n",
        "where users can run HQL statements. Also, the Hive shell also runs Hive JDBC and ODBC drivers and so\n",
        "can conduct queries from an Open Database Connectivity or Java Database Connectivity application.\n",
        "\n",
        "\n",
        "**What are the five different data types used by Apache Hive?:**\n",
        "**1)Numeric Data Types:**\n",
        "As the name suggests, these data types are integer-based data types. Examples of these data types are \n",
        "‘TINYINT,’ ‘SMALLINT,’ ‘INT,’ and ‘BIGINT’.\n",
        "\n",
        "**2)Date/Time Data Types:**\n",
        "These data types allow users to input a time and a date, with ‘TIMESTAMP,’ ‘DATE,’ and ‘INTERVAL,’ \n",
        "all being accepted inputs.\n",
        "\n",
        "**3)String Data Types:**\n",
        "Again this type of data is very straightforward and allows for written text, or ‘strings,’ to be \n",
        "implemented as data for processing. String data types include ‘STRING,’ ‘VARCHAR,’ and ‘CHAR.’\n",
        "\n",
        "**4)Complex Data Types:**\n",
        "One of the more advanced data types, complex types record more elaborate data and consist of types\n",
        " like ‘STRUCT’, ‘MAP,’ ‘ARRAY,’ and ‘UNION.’\n",
        "\n",
        "**5)Misc. Types:**\n",
        "Data types that don’t fit into any of the other four categories are known as miscellaneous data \n",
        "types and can take inputs such as ‘BOOLEAN’ or ‘BINARY.’"
      ],
      "metadata": {
        "id": "3ZhfVvyKHKXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hbase:\n",
        "- HBase is a column-oriented non-relational database management system that runs on top of Hadoop \n",
        "Distributed File System (HDFS). HBase provides a fault-tolerant way of storing sparse data sets,\n",
        " which are common in many big data use cases. It is well suited for real-time data processing or\n",
        " random read/write access to large volumes of data.\n",
        "- HBase is a data model that is similar to Google’s big table designed to provide quick random access\n",
        " to huge amounts of structured data. It leverages the fault tolerance provided by the Hadoop File\n",
        " System (HDFS).\n",
        "- It is a part of the Hadoop ecosystem that provides random real-time read/write access to data in \n",
        "the Hadoop File System"
      ],
      "metadata": {
        "id": "RO1f3gciId29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache kafka:\n",
        "Apache Kafka is an open-source distributed event streaming platform used by thousands of companies \n",
        "for high-performance data pipelines, streaming analytics, data integration, and mission-critical\n",
        " applications"
      ],
      "metadata": {
        "id": "o-WkmJQ6IpcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apache Snowflake:\n",
        "- Snowflake is a single platform comprised of storage, compute, and services layers that are logically\n",
        " integrated but scale infinitely and independent from one another.\n",
        "- Snowflake is a cloud data warehouse built on top of the public cloud (AWS/Azure / GCP) infrastructure\n",
        " and is a true SaaS offering. There is no hardware (virtual or physical) for you to select, install,\n",
        " configure, or manage. There is no software for you to install, configure, or manage. All ongoing\n",
        " maintenance, management, and tuning is handled by Snowflake."
      ],
      "metadata": {
        "id": "Rsb3TJJbIvP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Difference among data warehouse, data lake, Delta lake\n",
        "**1] DATA WAREHOUSE:**\n",
        "- Only Structured Data\n",
        "- Schema-on-Write\n",
        "- Supports ACID Transaction\n",
        "- Does not corrupt the system\n",
        "\n",
        "**2]Data Lake:**\n",
        "- Structured/Semi structured/Unstructured\n",
        "- Schema-on-Read\n",
        "- Minimal support to ACID Transactions\n",
        "- Leaves system in corrupted state\n",
        "\n",
        "**3]Delta lake:**\n",
        "- Structured/Semi structured/Unstructured/ Streaming\n",
        "- bSchema-on-Read\n",
        "- Supports ACID Transaction\n",
        "- Does not corrupt the system"
      ],
      "metadata": {
        "id": "jwYLhMJUI6tZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0h5zgXuuJX8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Formats:\n",
        "**1]CSV:** Good option for compatibility, spreadsheet processing and human readable data. The data must be \n",
        "flat. It is not efficient and cannot handle nested data. There may be issues with the separator which\n",
        "can lead to data quality issues. Use this format for exploratory analysis, POCs or small data sets.\n",
        "\n",
        "**2]JSON:** Heavily used in APIs. Nested format. It is widely adopted and human readable but it can be\n",
        "difficult to read if there are lots of nested fields. Great for small data sets, landing data or\n",
        "API integration. If possible convert to more efficient format before processing large amounts of data.\n",
        "\n",
        "**3]Avro:** Great for storing row data, very efficient. It has a schema and supports evolution. Great\n",
        "integration with Kafka. Supports file splitting. Use it for row level operations or in Kafka.\n",
        "Great to write data, slower to read.\n",
        "\n",
        "**4]Protocol Buffers:** Great for APIs, especially for gRPC. Supports Schema and it is very fast. Use \n",
        "for APIs or machine learning.\n",
        "\n",
        "**5]Parquet:** Columnar storage. It has schema support. It works very well with Hive and Spark as a\n",
        "way to store columnar data in deep storage that is queried using SQL. Because it stores data\n",
        "in columns, query engines will only read files that have the selected columns and not the entire \n",
        "data set as opposed to Avro. Use it as a reporting layer.\n",
        "\n",
        "**6]ORC:** Similar to Parquet, it offers better compression. It also provides better schema evolution\n",
        " support as well, but it is less popular.\n",
        "\n",
        "**1] Avro:**\n",
        "Schema Evolution -Best\n",
        "Compression - Good\n",
        "Splitability - Good\n",
        "Row or Column - Row\n",
        "Read or Write- Write\n",
        "\n",
        "**2] Parquet:**\n",
        "Schema Evolution -Good\n",
        "Compression - Better\n",
        "Splitability - Good\n",
        "Row or Column - Column\n",
        "Read or Write-  Read\n",
        "\n",
        "**3] ORC:**\n",
        "Schema Evolution -Better\n",
        "Compression - best\n",
        "Splitability - best\n",
        "Row or Column - column\n",
        "Read or Write- read"
      ],
      "metadata": {
        "id": "aYkW38BzKEF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ayw0pyOTJsJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hvN4XpsGCd-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G50cTfC6CL3s"
      }
    }
  ]
}